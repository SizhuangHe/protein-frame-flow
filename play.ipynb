{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Pytorch lightning imports\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from data.pdb_dataloader import PdbDataModule\n",
    "from models.flow_module import FlowModule\n",
    "from experiments import utils as eu\n",
    "cfg = OmegaConf.load(\"configs/base.yaml\")\n",
    "_cfg = cfg\n",
    "_data_cfg = cfg.data\n",
    "_exp_cfg = cfg.experiment\n",
    "_datamodule: LightningDataModule = PdbDataModule(_data_cfg)\n",
    "_datamodule.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"configs/base.yaml\")\n",
    "_cfg = cfg\n",
    "_data_cfg = cfg.data\n",
    "_exp_cfg = cfg.experiment\n",
    "_datamodule: LightningDataModule = PdbDataModule(_data_cfg)\n",
    "_datamodule.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = _datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['aatype', 'res_idx', 'rotmats_1', 'trans_1', 'res_mask', 'csv_idx'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 86, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = next(iter(train_loader))\n",
    "print(bs.keys())\n",
    "bs[\"trans_1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3, 19,  9, 16, 19, 18, 11,  3,  4,  2, 18, 16,  7, 13, 15,  7,  7, 10,\n",
       "         16,  9,  7,  3, 18,  2, 10,  0,  1, 10,  2, 15, 10,  7, 19, 10,  2,  3,\n",
       "          3,  9, 15, 15, 10,  1,  9, 16,  5,  7, 18,  5,  0,  9, 10, 18,  5,  3,\n",
       "          3,  2, 13,  7,  7,  0, 15, 16, 19,  9,  2, 15,  3,  2, 15,  4, 10,  2,\n",
       "         16, 16, 17,  2,  3, 11, 19, 15, 15,  9,  1, 19,  9,  0],\n",
       "        [11, 19, 13,  6, 19,  8, 19,  1, 14, 11, 11, 10,  0, 19,  6, 14, 11,  7,\n",
       "         15, 10,  6, 19,  2,  4, 15, 16, 16,  4,  2,  5, 14,  6, 19,  7,  7, 10,\n",
       "          6, 16, 15, 10,  2, 11,  9, 10, 10,  3,  6,  5,  0,  5, 17, 11,  8, 18,\n",
       "         10, 19, 15,  2,  9, 15,  8,  3, 16, 19, 10,  5,  4,  8, 13, 16,  4, 15,\n",
       "          7, 11,  5,  6, 15, 12,  2, 15,  2, 19, 15, 19, 18,  5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs[\"aatype\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm curious to see if ```res_mask``` are all 1's for all data points and it turns out they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3938/3938 [00:35<00:00, 109.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for bs in tqdm(train_loader):\n",
    "    res_mask = bs['res_mask']\n",
    "    assert not (res_mask != 1).any()\n",
    "\n",
    "# OK, all of res_mask contains all 1's\n",
    "# From the implementations, res_masks of value 1's don't make any difference, we can simplify some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample condition geodesic paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My interpolant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.my_interpolant import Interpolant \n",
    "interpolant = Interpolant(cfg.interpolant)\n",
    "interpolant.set_device(bs['res_mask'].device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```corrupt_batch``` to get trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing igso3_expansion: 100%|██████████| 1000/1000 [00:32<00:00, 30.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['aatype', 'res_idx', 'rotmats_1', 'trans_1', 'res_mask', 'csv_idx', 't', 'trans_t', 'rotmats_t'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_batch = interpolant.corrupt_batch(bs)\n",
    "noisy_batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among them, most useful ones are:\n",
    "- ```rotmats_t```: shape [B, l, max_num_res, 3, 3]\n",
    "- ```trans_t```: shape [B, l, max_num_res, 3]\n",
    "- ```t```: shape [B, l], the time points, created by interpolating 0 and 1\n",
    "- ```res_mask```: shape [B, l, max_num_res], mask of residues, generally all 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16, 128, 3, 3]),\n",
       " torch.Size([2, 16, 128, 3]),\n",
       " torch.Size([2, 16]),\n",
       " torch.Size([2, 16, 128]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_batch['rotmats_t'].shape, noisy_batch['trans_t'].shape, noisy_batch['t'].shape, noisy_batch['res_mask'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert 3-by-3 rotation matrices to 4d unit quaternions.\n",
    "\n",
    "Then, we convert the quaternions to 3d and concatenate with the 3d translation vector to form a 6d vector for each residue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of quaternions: torch.Size([2, 16, 128, 4])\n",
      "Processed quaternion shape: torch.Size([2, 16, 128, 3])\n",
      "Concatenated with translation vectors: torch.Size([2, 16, 128, 6])\n"
     ]
    }
   ],
   "source": [
    "import openfold.utils.rigid_utils as ru\n",
    "quats = ru.rot_to_quat(noisy_batch['rotmats_t'])\n",
    "print(\"shape of quaternions: {}\".format(quats.shape))\n",
    "\n",
    "# Step 1: Check if the first element in the last dimension is negative\n",
    "condition = quats[..., 0] < 0\n",
    "# Step 2: Conditionally negate the entire sub-tensor\n",
    "quats[condition] *= -1\n",
    "# Step 3: Discard the first element from the last dimension\n",
    "quats = quats[..., 1:] \n",
    "\n",
    "print(\"Processed quaternion shape: {}\".format(quats.shape))\n",
    "\n",
    "concat_quats_trans = torch.cat((quats, noisy_batch['trans_t']), dim=-1)\n",
    "print(\"Concatenated with translation vectors: {}\".format(concat_quats_trans.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate the vectors of each residue of a protein together to form a token for the protein backbone.\n",
    "- shape [B, l, 6*max_num_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone = concat_quats_trans.reshape(*concat_quats_trans.shape[:-2], -1)\n",
    "backbone.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vaellm_model import VAE_GPT2\n",
    "from transformers import GPT2Model\n",
    "gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "vae_gpt2 = VAE_GPT2(base_model=gpt2, emb_dim=768, z_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in model parameters: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['z_sampled', 'mu', 'log_sigma'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = vae_gpt2(backbone)\n",
    "llm_out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```z_sampled``` is the representation for the protein backbone. \n",
    "- shape [B, l, 6 * max_num_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out[\"z_sampled\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6961e+00, -1.2123e+00, -5.7993e+00,  ...,  3.0064e+00,\n",
       "           3.6463e+02, -1.0220e+01],\n",
       "         [-4.1796e+00, -1.0645e+00, -3.6344e+00,  ...,  2.8437e+00,\n",
       "           2.1208e+02, -3.0281e+00],\n",
       "         [-5.5020e+00, -1.0201e+00, -4.7652e+00,  ...,  6.5506e+00,\n",
       "           1.0661e+02,  3.3202e+00],\n",
       "         ...,\n",
       "         [-9.7807e-02, -7.8852e-01, -5.5312e+00,  ...,  2.2035e+00,\n",
       "           5.5355e+01, -3.3696e+00],\n",
       "         [-3.5365e+00, -8.2009e-01, -3.8279e+00,  ...,  1.6535e+00,\n",
       "           1.5321e+02, -6.3814e+00],\n",
       "         [-2.6931e+00, -8.7471e-01, -2.8582e+00,  ...,  1.6319e+00,\n",
       "           4.9120e+01, -3.4532e+00]],\n",
       "\n",
       "        [[-5.3179e+00, -1.9424e+00, -7.7379e+00,  ...,  5.8520e+00,\n",
       "           4.2114e+02,  4.9155e+00],\n",
       "         [-3.5607e+00, -1.9420e+00, -5.2748e+00,  ...,  5.7461e+00,\n",
       "          -8.3747e+02, -6.4710e+00],\n",
       "         [-4.3106e+00, -1.9539e+00, -5.3806e+00,  ...,  6.7742e+00,\n",
       "           1.8226e+03,  2.5803e+00],\n",
       "         ...,\n",
       "         [-3.9085e+00, -1.5046e+00, -6.3935e+00,  ...,  4.1424e+00,\n",
       "           6.5458e+02, -1.0757e+01],\n",
       "         [-4.3469e+00, -1.3839e+00, -5.5443e+00,  ...,  3.7189e+00,\n",
       "           1.0334e+02, -1.3450e+01],\n",
       "         [-3.3788e+00, -1.2590e+00, -5.3410e+00,  ...,  4.0076e+00,\n",
       "          -1.7804e+03, -6.2105e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out[\"z_sampled\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We unfold the tensor from backbone level back to residue level\n",
    "- shape [B, l, max_num_res, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 128, 6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, l, _ = llm_out[\"z_sampled\"].shape\n",
    "reshaped_z = llm_out[\"z_sampled\"].reshape(B, l, 128, -1)\n",
    "reshaped_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interpret the 6d vector for each residue back to 3d quaternion and 3d translation.\n",
    "\n",
    "We append a 1 to each quaternion to create 4d quaternions and normalize to form unit quaternions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovered 3D quaternions x,y,z of (1,x,y,z):torch.Size([2, 16, 128, 3])\n",
      "Recovered translation vectors: torch.Size([2, 16, 128, 3])\n",
      "-------\n",
      "Recovered 4D quaternions: torch.Size([2, 16, 128, 4])\n",
      "-------\n",
      "Recovered rotation matrices: torch.Size([2, 16, 128, 3, 3])\n",
      "Recovered translation vectors: torch.Size([2, 16, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "recovered_reduced_quats = reshaped_z[:, :, :, :3]\n",
    "recovered_trans = reshaped_z[:, :, :, 3:]\n",
    "print(f\"Recovered 3D quaternions x,y,z of (1,x,y,z):{recovered_reduced_quats.shape}\")\n",
    "print(f\"Recovered translation vectors: {recovered_trans.shape}\")\n",
    "print(\"-------\")\n",
    "\n",
    "ones = torch.ones(B, l, 128, 1)\n",
    "recovered_quats = torch.cat((ones, recovered_reduced_quats), dim=-1)\n",
    "recovered_quats = F.normalize(recovered_quats, p=2, dim=-1)\n",
    "print(f\"Recovered 4D quaternions: {recovered_quats.shape}\")\n",
    "print(\"-------\")\n",
    "\n",
    "recovered_rots = ru.quat_to_rot(recovered_quats)\n",
    "print(f\"Recovered rotation matrices: {recovered_rots.shape}\")\n",
    "print(f\"Recovered translation vectors: {recovered_trans.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would want to store them in a dictionary to make FrameDiff happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16, 128, 3, 3]),\n",
       " torch.Size([2, 16, 128, 3, 3]),\n",
       " torch.Size([2, 16, 128, 3]),\n",
       " torch.Size([2, 16, 128, 3]),\n",
       " torch.Size([2, 16]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_batch['processed_rotmats_t'] = recovered_rots\n",
    "noisy_batch['processed_trans_t'] = recovered_trans\n",
    "\n",
    "noisy_batch['processed_rotmats_t'].shape, noisy_batch['rotmats_t'].shape, noisy_batch['processed_trans_t'].shape, noisy_batch['trans_t'].shape, noisy_batch['t'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrameDiff work separately on each protein backbone. They expect a shape of [B, N, *], where * is 3,3 or 3 or other shapes for different variables.\n",
    "\n",
    "We make use of this. We collapse the minibatch and sequence dimension into one to trick FrameDiff into thinking this is the minibatch dimension. \n",
    "\n",
    "We can do this because our LLM-VAE already processed temporal dynamics and all we need to do now is to integrate spatially within each protein backbone.\n",
    "\n",
    "Important elements:\n",
    "- ```processed_rotmats_t```: shape [B * l, N, 3, 3]\n",
    "- ```processed_trans_t```: shape [B * l, N, 3]\n",
    "- ```t```: shape [B * l, 1]. Note FrameDiff expects one time point for each protein. This is also true for us.\n",
    "- ```res_mask```: shape [B * l, max_num_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128, 3, 3]),\n",
       " torch.Size([32, 128, 3]),\n",
       " torch.Size([32, 1]),\n",
       " torch.Size([32, 128]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, l, N, _, _ = noisy_batch['rotmats_t'].shape\n",
    "noisy_batch['processed_rotmats_t'] = noisy_batch['processed_rotmats_t'].reshape(B*l, N, 3, 3)\n",
    "noisy_batch['processed_trans_t'] = noisy_batch['processed_trans_t'].reshape(B*l, N, 3)\n",
    "noisy_batch['t'] = noisy_batch['t'].reshape(B*l, 1)\n",
    "noisy_batch['res_mask'] = noisy_batch['res_mask'].reshape(B*l, -1)\n",
    "\n",
    "noisy_batch['processed_rotmats_t'].shape, noisy_batch['processed_trans_t'].shape, noisy_batch['t'].shape, noisy_batch['res_mask'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we processed everything, FrameDiff is happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dev_flow_model import FlowModel\n",
    "model = FlowModel(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pred_trans', 'pred_rotmats'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framediff_out = model(noisy_batch)\n",
    "framediff_out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are\n",
    "- ```pred_rotmats```: shape [B * l, max_num_res, 3, 3]\n",
    "- ```pred_trans```: shape [B * l, max_num_res, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128, 3, 3]), torch.Size([32, 128, 3]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framediff_out['pred_rotmats'].shape, framediff_out['pred_trans'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we would want to compute loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128, 3, 3]),\n",
       " torch.Size([32, 128, 3, 3]),\n",
       " torch.Size([32, 128, 3]),\n",
       " torch.Size([32, 128, 3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_batch[\"processed_rotmats_t\"].shape, framediff_out['pred_rotmats'].shape, noisy_batch[\"processed_trans_t\"].shape, framediff_out['pred_trans'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_plddt_mask': None, 'loss': 'auxiliary_loss', 'bb_atom_scale': 0.1, 'trans_scale': 0.1, 'translation_loss_weight': 2.0, 't_normalize_clip': 0.9, 'rotation_loss_weights': 1.0, 'aux_loss_weight': 1.0, 'aux_loss_t_pass': 0.25}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_exp_cfg = cfg.experiment\n",
    "training_cfg = _exp_cfg.training\n",
    "training_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mask = noisy_batch['res_mask']\n",
    "\n",
    "if training_cfg.min_plddt_mask is not None:\n",
    "    plddt_mask = noisy_batch['res_plddt'] > training_cfg.min_plddt_mask\n",
    "    loss_mask *= plddt_mask\n",
    "\n",
    "num_batch, num_res = loss_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We throw away the first time point (t = 0) here. This is because later we need to shift tokens because that's what we do in CausalLM tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of time points: torch.Size([32, 1])\n",
      "Processed time step shape: torch.Size([30, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 1]), torch.Size([30, 1, 1]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Timestep used for normalization.\n",
    "print(f\"Shape of time points: {noisy_batch['t'].shape}\")\n",
    "t = noisy_batch['t'].reshape(B, l, 1)[:, 1:, :].reshape(-1,1)\n",
    "print(f\"Processed time step shape: {t.shape}\")\n",
    "norm_scale = 1 - torch.min(\n",
    "    t[..., None], torch.tensor(training_cfg.t_normalize_clip))\n",
    "\n",
    "t.shape, norm_scale.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the model outputs, the shapes are [B*l, max_num_res, *].\n",
    "\n",
    "We have the sampled ground truths trajectories, the shapes are [B, l, max_num_res, *].\n",
    "\n",
    "For CausalLM tasks, we need to shift the predicted and ground truth tokens so we first unfold them to [B, l, max_num_res, *]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16, 128, 3, 3]),\n",
       " torch.Size([2, 16, 128, 3, 3]),\n",
       " torch.Size([2, 16, 128, 3]),\n",
       " torch.Size([2, 16, 128, 3]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the model outputs\n",
    "pred_trans = framediff_out['pred_trans'].reshape(B, l, 128, 3)\n",
    "pred_rotmats = framediff_out['pred_rotmats'].reshape(B, l, 128, 3, 3)\n",
    "\n",
    "# These are the sampled ground truths.\n",
    "gt_trans = noisy_batch['trans_t']\n",
    "gt_rotmats = noisy_batch['rotmats_t']\n",
    "\n",
    "pred_rotmats.shape, gt_rotmats.shape, pred_trans.shape, gt_trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we shift the prediction and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 15, 128, 3, 3]),\n",
       " torch.Size([2, 15, 128, 3, 3]),\n",
       " torch.Size([2, 15, 128, 3]),\n",
       " torch.Size([2, 15, 128, 3]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_pred_trans  = pred_trans[:, :-1, :, :]\n",
    "shifted_pred_rotmats = pred_rotmats[:, :-1, :, :, :]\n",
    "shifted_gt_trans = gt_trans[:, 1:, :, :]\n",
    "shifted_gt_rotmats = gt_rotmats[:, 1:, :, :, :]\n",
    "\n",
    "shifted_pred_rotmats.shape, shifted_gt_rotmats.shape, shifted_pred_trans.shape, shifted_gt_trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we collapse the first two dimensions to make ```all_atom``` happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 128, 3, 3]),\n",
       " torch.Size([30, 128, 3, 3]),\n",
       " torch.Size([30, 128, 3]),\n",
       " torch.Size([30, 128, 3]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_shifted_pred_trans = shifted_pred_trans.reshape(B*(l-1), 128, 3)\n",
    "flat_shifted_pred_rotmats = shifted_pred_rotmats.reshape(B*(l-1), 128, 3, 3)\n",
    "\n",
    "flat_shifted_gt_trans = shifted_gt_trans.reshape(B*(l-1), 128, 3)\n",
    "flat_shifted_gt_rotmats = shifted_gt_rotmats.reshape(B*(l-1), 128, 3, 3)\n",
    "\n",
    "flat_shifted_pred_rotmats.shape, flat_shifted_gt_rotmats.shape, flat_shifted_pred_trans.shape, flat_shifted_gt_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import all_atom\n",
    "gt_bb_atoms = all_atom.to_atom37(flat_shifted_gt_trans, flat_shifted_gt_rotmats)[:, :, :3] \n",
    "pred_bb_atoms = all_atom.to_atom37(flat_shifted_pred_trans, flat_shifted_pred_rotmats)[:, :, :3]\n",
    "\n",
    "gt_bb_atoms *= training_cfg.bb_atom_scale / norm_scale[..., None]\n",
    "pred_bb_atoms *= training_cfg.bb_atom_scale / norm_scale[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 128, 3, 3]), torch.Size([30, 128, 3, 3]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_bb_atoms.shape, pred_bb_atoms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_denom = torch.sum(loss_mask, dim=-1, dtype=torch.float).mean() * 3\n",
    "bb_atom_loss = torch.sum(\n",
    "    (gt_bb_atoms - pred_bb_atoms) ** 2,\n",
    "    dim=(-1, -2, -3)\n",
    ") / loss_denom\n",
    "\n",
    "bb_atom_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 384, 384]) torch.Size([30, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "num_batch = gt_bb_atoms.shape[0]\n",
    "# Pairwise distance loss\n",
    "gt_flat_atoms = gt_bb_atoms.reshape([num_batch, num_res*3, 3])\n",
    "gt_pair_dists = torch.linalg.norm(\n",
    "    gt_flat_atoms[:, :, None, :] - gt_flat_atoms[:, None, :, :], dim=-1)\n",
    "pred_flat_atoms = pred_bb_atoms.reshape([num_batch, num_res*3, 3])\n",
    "pred_pair_dists = torch.linalg.norm(\n",
    "    pred_flat_atoms[:, :, None, :] - pred_flat_atoms[:, None, :, :], dim=-1)\n",
    "\n",
    "print(gt_pair_dists.shape, pred_pair_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_loss_mask = torch.tile(loss_mask[:, :, None], (1, 1, 3))[B:,:,:] # change the shape because we shifted tokens, all entries of loss masks are 1 so don't matter, we throw away B tokens\n",
    "flat_loss_mask = flat_loss_mask.reshape([num_batch, num_res*3])\n",
    "flat_res_mask = torch.tile(loss_mask[:, :, None], (1, 1, 3))[B:,:,:] # change the shape because we shifted tokens, all entries of loss masks are 1 so don't matter\n",
    "flat_res_mask = flat_res_mask.reshape([num_batch, num_res*3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 384]), torch.Size([30, 384]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_loss_mask.shape, flat_res_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_pair_dists = gt_pair_dists * flat_loss_mask[..., None]\n",
    "pred_pair_dists = pred_pair_dists * flat_loss_mask[..., None]\n",
    "pair_dist_mask = flat_loss_mask[..., None] * flat_res_mask[:, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat_loss = torch.sum(\n",
    "    (gt_pair_dists - pred_pair_dists)**2 * pair_dist_mask,\n",
    "    dim=(1, 2))\n",
    "dist_mat_loss /= (torch.sum(pair_dist_mask, dim=(1, 2)) - num_res)\n",
    "\n",
    "auxiliary_loss = (bb_atom_loss + dist_mat_loss) * (\n",
    "    t[:, 0]> training_cfg.aux_loss_t_pass\n",
    ")\n",
    "auxiliary_loss *= _exp_cfg.training.aux_loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8200e+02, 2.6897e+02, 2.9347e+02,\n",
       "        2.8871e+02, 2.5675e+02, 4.1631e+02, 6.7745e+02, 1.1647e+03, 2.1560e+03,\n",
       "        4.0746e+03, 5.8246e+03, 7.0865e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.0443e+04, 1.2287e+04, 2.4929e+04, 1.6558e+04, 1.3572e+04, 1.7375e+04,\n",
       "        3.1194e+04, 3.6437e+04, 1.0511e+05, 3.7607e+05, 3.2303e+05, 2.4006e+05],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxiliary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div = (1 + 2 * llm_out[\"log_sigma\"] - llm_out[\"mu\"].pow(2) - llm_out[\"log_sigma\"].exp().pow(2))[:, :-1, :] # Throw away the last mu and sigma because we're not using it to predict\n",
    "kl_div = - 0.5 * kl_div.sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = F.mse_loss(flat_shifted_pred_trans, flat_shifted_gt_trans) + F.mse_loss(flat_shifted_pred_rotmats, flat_shifted_gt_rotmats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse_loss+auxiliary_loss.sum()+kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12136346., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.my_flow_module import FlowModule\n",
    "\n",
    "_datamodule: LightningDataModule = PdbDataModule(_data_cfg)\n",
    "_model: LightningModule = FlowModule(_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/py ...\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: `Fabric(strategy='dp'|'ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mGPUtil\u001b[39;00m\n\u001b[1;32m      5\u001b[0m devices \u001b[38;5;241m=\u001b[39m GPUtil\u001b[38;5;241m.\u001b[39mgetAvailable(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m'\u001b[39m, limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m)[:_exp_cfg\u001b[38;5;241m.\u001b[39mnum_devices]\n\u001b[0;32m----> 6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_exp_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#callbacks=callbacks,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# logger=logger,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_model_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m     17\u001b[0m     datamodule\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datamodule,\n\u001b[1;32m     18\u001b[0m     ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_cfg\u001b[38;5;241m.\u001b[39mwarm_start\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m     69\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:399\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m _DataConnector(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 399\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m _AcceleratorConnector(\n\u001b[1;32m    400\u001b[0m     devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[1;32m    401\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[1;32m    402\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m    403\u001b[0m     num_nodes\u001b[39m=\u001b[39;49mnum_nodes,\n\u001b[1;32m    404\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49msync_batchnorm,\n\u001b[1;32m    405\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[1;32m    406\u001b[0m     use_distributed_sampler\u001b[39m=\u001b[39;49muse_distributed_sampler,\n\u001b[1;32m    407\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m    408\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[1;32m    409\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[1;32m    410\u001b[0m )\n\u001b[1;32m    411\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector \u001b[39m=\u001b[39m _LoggerConnector(\u001b[39mself\u001b[39m)\n\u001b[1;32m    412\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector \u001b[39m=\u001b[39m _CallbackConnector(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:173\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_and_init_precision()\n\u001b[1;32m    172\u001b[0m \u001b[39m# 6. Instantiate Strategy - Part 2\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy_init_strategy()\n",
      "File \u001b[0;32m/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:576\u001b[0m, in \u001b[0;36m_AcceleratorConnector._lazy_init_strategy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_configure_launcher()\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m _IS_INTERACTIVE \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mis_interactive_compatible:\n\u001b[0;32m--> 576\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    577\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer(strategy=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy_flag\u001b[39m!r}\u001b[39;00m\u001b[39m)` is not compatible with an interactive\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m environment. Run your code as a script, or choose one of the compatible strategies:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `Fabric(strategy=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39mddp_notebook\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m In case you are spawning processes yourself, make sure to include the Trainer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m creation inside the worker function.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n\u001b[1;32m    584\u001b[0m \u001b[39m# TODO: should be moved to _check_strategy_and_fallback().\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[39m# Current test check precision first, so keep this check here to meet error order\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, TPUAccelerator) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    587\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy, (SingleTPUStrategy, XLAStrategy)\n\u001b[1;32m    588\u001b[0m ):\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: `Fabric(strategy='dp'|'ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "# logger = WandbLogger(\n",
    "#                 _exp_cfg.wandb,\n",
    "#             )\n",
    "import GPUtil\n",
    "devices = GPUtil.getAvailable(order='memory', limit = 8)[:_exp_cfg.num_devices]\n",
    "trainer = Trainer(\n",
    "    **_exp_cfg.trainer,\n",
    "    #callbacks=callbacks,\n",
    "    # logger=logger,\n",
    "    use_distributed_sampler=False,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    "    devices=devices,\n",
    ")\n",
    "trainer.fit(\n",
    "    model=self._model,\n",
    "    datamodule=self._datamodule,\n",
    "    ckpt_path=self._exp_cfg.warm_start\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(\n",
    "            self,\n",
    "            num_batch,\n",
    "            num_res,\n",
    "            model,\n",
    "        ):\n",
    "        res_mask = torch.ones(num_batch, num_res, device=self._device)\n",
    "\n",
    "        # Set-up initial prior samples\n",
    "        trans_0 = _centered_gaussian(\n",
    "            num_batch, num_res, self._device) * du.NM_TO_ANG_SCALE\n",
    "        rotmats_0 = _uniform_so3(num_batch, num_res, self._device)\n",
    "        batch = {\n",
    "            'res_mask': res_mask,\n",
    "        }\n",
    "\n",
    "        # Set-up time\n",
    "        ts = torch.linspace(\n",
    "            self._cfg.min_t, 1.0, self._sample_cfg.num_timesteps)\n",
    "        t_1 = ts[0]\n",
    "\n",
    "        prot_traj = [(trans_0, rotmats_0)]\n",
    "        clean_traj = []\n",
    "        for t_2 in ts[1:]:\n",
    "\n",
    "            # Run model.\n",
    "            trans_t_1, rotmats_t_1 = prot_traj[-1]\n",
    "            batch['trans_t'] = trans_t_1\n",
    "            batch['rotmats_t'] = rotmats_t_1\n",
    "            t = torch.ones((num_batch, 1), device=self._device) * t_1\n",
    "            batch['t'] = t\n",
    "            with torch.no_grad():\n",
    "                model_out = model(batch)\n",
    "\n",
    "            # Process model output.\n",
    "            pred_trans_1 = model_out['pred_trans']\n",
    "            pred_rotmats_1 = model_out['pred_rotmats']\n",
    "            clean_traj.append(\n",
    "                (pred_trans_1.detach().cpu(), pred_rotmats_1.detach().cpu())\n",
    "            )\n",
    "            if self._cfg.self_condition:\n",
    "                batch['trans_sc'] = pred_trans_1\n",
    "\n",
    "            # Take reverse step\n",
    "            d_t = t_2 - t_1\n",
    "            trans_t_2 = self._trans_euler_step(\n",
    "                d_t, t_1, pred_trans_1, trans_t_1)\n",
    "            rotmats_t_2 = self._rots_euler_step(\n",
    "                d_t, t_1, pred_rotmats_1, rotmats_t_1)\n",
    "            prot_traj.append((trans_t_2, rotmats_t_2))\n",
    "            t_1 = t_2\n",
    "\n",
    "        # We only integrated to min_t, so need to make a final step\n",
    "        t_1 = ts[-1]\n",
    "        trans_t_1, rotmats_t_1 = prot_traj[-1]\n",
    "        batch['trans_t'] = trans_t_1\n",
    "        batch['rotmats_t'] = rotmats_t_1\n",
    "        batch['t'] = torch.ones((num_batch, 1), device=self._device) * t_1\n",
    "        with torch.no_grad():\n",
    "            model_out = model(batch)\n",
    "        pred_trans_1 = model_out['pred_trans']\n",
    "        pred_rotmats_1 = model_out['pred_rotmats']\n",
    "        clean_traj.append(\n",
    "            (pred_trans_1.detach().cpu(), pred_rotmats_1.detach().cpu())\n",
    "        )\n",
    "        prot_traj.append((pred_trans_1, pred_rotmats_1))\n",
    "\n",
    "        # Convert trajectories to atom37.\n",
    "        atom37_traj = all_atom.transrot_to_atom37(prot_traj, res_mask)\n",
    "        clean_atom37_traj = all_atom.transrot_to_atom37(clean_traj, res_mask)\n",
    "        return atom37_traj, clean_atom37_traj, clean_traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Pytorch lightning imports\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from data.pdb_dataloader import PdbDataModule\n",
    "from models.flow_module import FlowModule\n",
    "from experiments import utils as eu\n",
    "cfg = OmegaConf.load(\"configs/base.yaml\")\n",
    "_cfg = cfg\n",
    "_data_cfg = cfg.data\n",
    "_exp_cfg = cfg.experiment\n",
    "_datamodule: LightningDataModule = PdbDataModule(_data_cfg)\n",
    "_datamodule.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import utils as du\n",
    "from scipy.spatial.transform import Rotation\n",
    "def _centered_gaussian(num_batch, num_res, device):\n",
    "    noise = torch.randn(num_batch, num_res, 3, device=device)\n",
    "    return noise - torch.mean(noise, dim=-2, keepdims=True)\n",
    "\n",
    "def _uniform_so3(num_batch, num_res, device):\n",
    "    return torch.tensor(\n",
    "        Rotation.random(num_batch*num_res).as_matrix(),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    ).reshape(num_batch, num_res, 3, 3)\n",
    "\n",
    "num_batch=2\n",
    "num_res=20\n",
    "_device = \"cuda\"\n",
    "res_mask = torch.ones(num_batch, num_res, device=_device)\n",
    "trans_0 = _centered_gaussian(\n",
    "            num_batch, num_res, _device) * du.NM_TO_ANG_SCALE\n",
    "rotmats_0 = _uniform_so3(num_batch, num_res, _device)\n",
    "batch = {\n",
    "    'res_mask': res_mask,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_0 = trans_0[:, :, None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20, 1, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_0.shape # [B, N, 3] -> [B, N, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotmats_0 = rotmats_0[:,:,None,:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20, 1, 3, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotmats_0.shape # [B, N, 3, 3] -> [B, N, 1, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = {\n",
    "            'res_mask': res_mask,\n",
    "        }\n",
    "t = torch.linspace(0,1,16)[None, :].repeat(num_batch,1).to(\"cuda\")\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def _pad_trans(trans_t):\n",
    "    '''\n",
    "            Pad rotmats_t from [B,N,l,3], \n",
    "                where N is the actual number of residues (and N \\leq max_num_res) \n",
    "            to [B, max_num_res,l,3] with all 0's\n",
    "    '''\n",
    "    trans_t_padded = F.pad(trans_t, (0, 0, 0, 0, 0, 128 - trans_t.shape[1]), \"constant\", 0)\n",
    "    return trans_t_padded\n",
    "\n",
    "def _pad_rotmats(rotmats_t):\n",
    "    '''\n",
    "        Pad rotmats_t from [B,N,l,3,3], \n",
    "                where N is the actual number of residues (and N \\leq max_num_res) \n",
    "            to [B, max_num_res,l,3,3] with all 0's\n",
    "    '''\n",
    "    rotmats_t_padded = F.pad(rotmats_t, (0, 0, 0, 0, 0, 0, 0, 128 - rotmats_t.shape[1]), \"constant\", 0)\n",
    "    return rotmats_t_padded\n",
    "\n",
    "def _pad_res_mask(self, res_mask):\n",
    "    '''\n",
    "            Pad rotmats from [B,N],\n",
    "                where N is the actual number of residues (and N \\leq max_num_res) \n",
    "            to [B, max_num_res] with all 1's\n",
    "\n",
    "            Note: pad with 1's not 0's\n",
    "    '''\n",
    "    res_mask_padded = F.pad(res_mask, (0, 128 - res_mask.shape[1]), mode='constant', value=1)\n",
    "    return res_mask_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 1, 3, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotmats_0 = _pad_rotmats(rotmats_0)\n",
    "rotmats_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 1, 3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_0 = _pad_trans(trans_0)\n",
    "trans_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 3, 3])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotmats_0 = rotmats_0.permute(0, 2, 1, 3, 4)\n",
    "rotmats_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 3])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_0 = trans_0.permute(0, 2, 1, 3)\n",
    "trans_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['res_mask'] = batch['res_mask'][:,None,:].repeat(1, 16, 1) # replace 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"trans_t\"] = trans_0 \n",
    "batch[\"rotmats_t\"] = rotmats_0\n",
    "batch['t'] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 128, 3]), torch.Size([2, 1, 128, 3, 3]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"trans_t\"].shape, batch[\"rotmats_t\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.together_model import ProteinVAELLMmodel\n",
    "model = ProteinVAELLMmodel(cfg).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in model parameters: []\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():   \n",
    "    out = model.generate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pred_trans', 'pred_rotmats'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = num_batch\n",
    "l = 16\n",
    "N = num_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"pred_trans\"] = out[\"pred_trans\"].reshape(B, l, N, 3)\n",
    "out[\"pred_rotmats\"] = out[\"pred_rotmats\"].reshape(B, l, N, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16, 20, 3]), torch.Size([2, 16, 20, 3, 3]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"pred_trans\"].shape, out[\"pred_rotmats\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_trajectory = []\n",
    "for i in range(out[\"pred_trans\"].shape[1]):\n",
    "    protein_trajectory.append(\n",
    "        (out[\"pred_trans\"][:, i, :, :].detach().cpu(), out[\"pred_rotmats\"][:, i, :, :, :].detach().cpu())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import all_atom\n",
    "atom37_traj = all_atom.transrot_to_atom37(protein_trajectory, res_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20, 37, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom37_traj[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in model parameters: []\n"
     ]
    }
   ],
   "source": [
    "from data.my_interpolant import Interpolant \n",
    "interpolant = Interpolant(cfg.interpolant)\n",
    "interpolant.set_device(\"cuda\")\n",
    "out_sample = interpolant.sample(num_batch=2, num_res=20, model=model)[0][-1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20, 37, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "gpt2 = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit ('fm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0e2400c1a377b47623d27da16e17b3eddff11c79be96fbe43a1e38910d94694"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
