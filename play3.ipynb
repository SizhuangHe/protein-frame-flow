{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x148b9e11ebc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from models.together_model import ProteinVAELLMmodel, ProteinVAELLM_FrameDiff_first_model\n",
    "from data import all_atom\n",
    "# Pytorch lightning imports\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from data.pdb_dataloader import PdbDataModule\n",
    "from models.flow_module import FlowModule\n",
    "from experiments import utils as eu\n",
    "import openfold.utils.rigid_utils as ru\n",
    "import torch.nn.functional as F\n",
    "cfg = OmegaConf.load(\"configs/base.yaml\")\n",
    "_cfg = cfg\n",
    "_data_cfg = cfg.data\n",
    "_exp_cfg = cfg.experiment\n",
    "_datamodule: LightningDataModule = PdbDataModule(_data_cfg)\n",
    "_datamodule.setup(stage=\"fit\")\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set stuff up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"configs/base.yaml\")\n",
    "_cfg = cfg\n",
    "_data_cfg = cfg.data\n",
    "_exp_cfg = cfg.experiment\n",
    "training_cfg = _exp_cfg.training\n",
    "_datamodule: LightningDataModule = PdbDataModule(_data_cfg)\n",
    "_datamodule.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = _datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.my_interpolant import Interpolant \n",
    "interpolant = Interpolant(cfg.interpolant)\n",
    "interpolant.set_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProteinVAELLM_FrameDiff_first_model(cfg).to(\"cuda\")\n",
    "model.attach_backward_hooks() # attach backward hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "            params=model.parameters(),\n",
    "            lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProteinVAELLM_FrameDiff_first_model(\n",
       "  (framediff_model): FlowModel(\n",
       "    (node_embedder): NodeEmbedder(\n",
       "      (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "    (edge_embedder): EdgeEmbedder(\n",
       "      (linear_s_p): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (linear_relpos): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (edge_embedder): Sequential(\n",
       "        (0): Linear(in_features=236, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (trunk): ModuleDict(\n",
       "      (ipa_0): InvariantPointAttention(\n",
       "        (linear_q): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (linear_kv): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (linear_q_points): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (linear_kv_points): Linear(in_features=128, out_features=480, bias=True)\n",
       "        (linear_b): Linear(in_features=64, out_features=8, bias=True)\n",
       "        (down_z): Linear(in_features=64, out_features=16, bias=True)\n",
       "        (linear_out): Linear(in_features=1536, out_features=128, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (ipa_ln_0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (seq_tfmr_0): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_tfmr_0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (node_transition_0): StructureModuleTransition(\n",
       "        (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (linear_3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (bb_update_0): BackboneUpdate(\n",
       "        (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vaellm_model): VAE_GPT2(\n",
       "    (llm_model): GPT2Model(\n",
       "      (wte): None\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (hid2mu): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (hid2sigma): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (lin_emb_backbone): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (lin_deembed_backbone): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2, number of time points: 16, number of residues: 86\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "# print(batch[\"aatype\"])\n",
    "for key, value in batch.items():\n",
    "    batch[key] = value.to(\"cuda\")\n",
    "noisy_batch = interpolant.corrupt_batch(batch, pad=False)\n",
    "B, l, _ = noisy_batch['res_mask'].shape\n",
    "num_res = noisy_batch[\"aatype\"].shape[1]\n",
    "print(f\"Batch size: {B}, number of time points: {l}, number of residues: {num_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigvals: tensor([[[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         ...,\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "        [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         ...,\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "        [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         ...,\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         ...,\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "        [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         ...,\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "        [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         ...,\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "         [-0.3333, -0.3333, -0.3333,  1.0000]]], device='cuda:0')\n",
      "k requires grad: False\n",
      "Inside rot to quat: vector requires grad: False\n",
      "Inside rot to quat: quats requires grad: False\n",
      "Inside rot to quat: vector shape: torch.Size([32, 86, 4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1554: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'dict'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigvals: tensor([[[[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "         [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "         [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "         [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "         [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "         [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "         [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "         [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]],\n",
      "\n",
      "         [[-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000],\n",
      "          [-0.3333, -0.3333, -0.3333,  1.0000]]]], device='cuda:0',\n",
      "       grad_fn=<LinalgEighBackward0>)\n",
      "k requires grad: True\n",
      "Inside rot to quat: vector requires grad: True\n",
      "Inside rot to quat: quats requires grad: True\n",
      "Inside rot to quat: vector shape: torch.Size([2, 16, 86, 4, 4])\n",
      "Quats requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1554: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    }
   ],
   "source": [
    "loss_mask = noisy_batch['res_mask'].reshape(B*l, num_res)\n",
    "        \n",
    "if training_cfg.min_plddt_mask is not None:\n",
    "    plddt_mask = noisy_batch['res_plddt'] > training_cfg.min_plddt_mask\n",
    "    loss_mask *= plddt_mask\n",
    "        \n",
    "\n",
    "# Ground truth labels\n",
    "gt_trans = noisy_batch['trans_t']\n",
    "gt_rotmats = noisy_batch['rotmats_t']\n",
    "\n",
    "# Model output predictions.\n",
    "\n",
    "framediff_out = model(noisy_batch)\n",
    "pred_trans = framediff_out[\"pred_T\"]['pred_trans']\n",
    "pred_rotmats = framediff_out[\"pred_T\"]['pred_rotmats']\n",
    "        \n",
    "# Shift for CausalLM\n",
    "shifted_pred_trans  = pred_trans[:, :-1, :, :]\n",
    "shifted_pred_rotmats = pred_rotmats[:, :-1, :, :, :]\n",
    "shifted_gt_trans = gt_trans[:, 1:, :, :]\n",
    "shifted_gt_rotmats = gt_rotmats[:, 1:, :, :, :]\n",
    "\n",
    "# Reshape back to [B*(l-1), 128, *]\n",
    "flat_shifted_pred_trans = shifted_pred_trans.reshape(B*(l-1), num_res, 3)\n",
    "flat_shifted_pred_rotmats = shifted_pred_rotmats.reshape(B*(l-1), num_res, 3, 3)\n",
    "\n",
    "flat_shifted_gt_trans = shifted_gt_trans.reshape(B*(l-1), num_res, 3)\n",
    "flat_shifted_gt_rotmats = shifted_gt_rotmats.reshape(B*(l-1), num_res, 3, 3)\n",
    "\n",
    "# Timestep used for normalization.\n",
    "t = noisy_batch['t'].reshape(B, l, 1)[:, 1:, :].reshape(-1,1) # We throw away the first time points\n",
    "norm_scale = 1 - torch.min(\n",
    "    t[..., None], torch.tensor(training_cfg.t_normalize_clip))\n",
    "        \n",
    "        \n",
    "\n",
    "# Backbone atom loss\n",
    "gt_bb_atoms = all_atom.to_atom37(flat_shifted_gt_trans, flat_shifted_gt_rotmats)[:, :, :3] \n",
    "pred_bb_atoms = all_atom.to_atom37(flat_shifted_pred_trans, flat_shifted_pred_rotmats)[:, :, :3]\n",
    "\n",
    "gt_bb_atoms *= training_cfg.bb_atom_scale / norm_scale[..., None]\n",
    "pred_bb_atoms *= training_cfg.bb_atom_scale / norm_scale[..., None]\n",
    "                \n",
    "        \n",
    "loss_denom = torch.sum(loss_mask, dim=-1, dtype=torch.float).mean() * 3 # Added a mean here, this doesn'y matter since our mask is all 1's\n",
    "bb_atom_loss = torch.sum(\n",
    "    (gt_bb_atoms - pred_bb_atoms) ** 2,\n",
    "    dim=(-1, -2, -3)\n",
    ") / loss_denom\n",
    "\n",
    "# Pairwise distance loss\n",
    "num_batch = gt_bb_atoms.shape[0]\n",
    "gt_flat_atoms = gt_bb_atoms.reshape([num_batch, num_res*3, 3])\n",
    "gt_pair_dists = torch.linalg.norm(\n",
    "    gt_flat_atoms[:, :, None, :] - gt_flat_atoms[:, None, :, :], dim=-1)\n",
    "pred_flat_atoms = pred_bb_atoms.reshape([num_batch, num_res*3, 3])\n",
    "pred_pair_dists = torch.linalg.norm(\n",
    "    pred_flat_atoms[:, :, None, :] - pred_flat_atoms[:, None, :, :], dim=-1)\n",
    "\n",
    "flat_loss_mask = torch.tile(loss_mask[:, :, None], (1, 1, 3))[B:,:,:] # change the shape because we shifted tokens, all entries of loss masks are 1 so don't matter, we throw away B tokens\n",
    "flat_loss_mask = flat_loss_mask.reshape([num_batch, num_res*3])\n",
    "flat_res_mask = torch.tile(loss_mask[:, :, None], (1, 1, 3))[B:,:,:] # change the shape because we shifted tokens, all entries of loss masks are 1 so don't matter\n",
    "flat_res_mask = flat_res_mask.reshape([num_batch, num_res*3])\n",
    "\n",
    "gt_pair_dists = gt_pair_dists * flat_loss_mask[..., None]\n",
    "pred_pair_dists = pred_pair_dists * flat_loss_mask[..., None]\n",
    "pair_dist_mask = flat_loss_mask[..., None] * flat_res_mask[:, None, :]\n",
    "\n",
    "dist_mat_loss = torch.sum(\n",
    "    (gt_pair_dists - pred_pair_dists)**2 * pair_dist_mask,\n",
    "    dim=(1, 2))\n",
    "dist_mat_loss /= (torch.sum(pair_dist_mask, dim=(1, 2)) - num_res)\n",
    "\n",
    "auxiliary_loss = (bb_atom_loss + dist_mat_loss) * (\n",
    "    t[:, 0]> training_cfg.aux_loss_t_pass\n",
    ")\n",
    "auxiliary_loss *= _exp_cfg.training.aux_loss_weight\n",
    "        \n",
    "auxiliary_loss = auxiliary_loss.mean()\n",
    "kl_div = (1 + 2 * framediff_out[\"vae_log_sigma\"] - framediff_out[\"vae_mu\"].pow(2) - framediff_out[\"vae_log_sigma\"].exp().pow(2))[:, :-1, :] # Throw away the last mu and sigma because we're not using it to predict\n",
    "kl_div = - 0.5 * kl_div.sum(dim=-1).mean()\n",
    "mse_loss = F.mse_loss(flat_shifted_pred_trans, flat_shifted_gt_trans) + F.mse_loss(flat_shifted_pred_rotmats, flat_shifted_gt_rotmats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3333295.2500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "No NaNs detected in any gradient inputs of Linear\n",
      "No NaNs detected in any gradient outputs of Linear\n",
      "No NaNs detected in any gradient inputs of Linear\n",
      "No NaNs detected in any gradient outputs of Linear\n",
      "No NaNs detected in any gradient inputs of Linear\n",
      "No NaNs detected in any gradient outputs of Linear\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of NewGELUActivation\n",
      "No NaNs detected in any gradient outputs of NewGELUActivation\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2MLP\n",
      "No NaNs detected in any gradient outputs of GPT2MLP\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Conv1D\n",
      "No NaNs detected in any gradient outputs of Conv1D\n",
      "No NaNs detected in any gradient inputs of GPT2Attention\n",
      "No NaNs detected in any gradient outputs of GPT2Attention\n",
      "No NaNs detected in any gradient inputs of LayerNorm\n",
      "No NaNs detected in any gradient outputs of LayerNorm\n",
      "No NaNs detected in any gradient inputs of GPT2Block\n",
      "No NaNs detected in any gradient outputs of GPT2Block\n",
      "No NaNs detected in any gradient inputs of Dropout\n",
      "No NaNs detected in any gradient outputs of Dropout\n",
      "No NaNs detected in any gradient inputs of Embedding\n",
      "No NaNs detected in any gradient outputs of Embedding\n",
      "No NaNs detected in any gradient inputs of Linear\n",
      "No NaNs detected in any gradient outputs of Linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: Error detected in LinalgEighBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/tmp.0dj7NeJ4Z1/ipykernel_2165971/2035242101.py\", line 14, in <module>\n",
      "    framediff_out = model(noisy_batch)\n",
      "  File \"/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/vast/palmer/home.mccleary/sh2748/protein-frame-flow/models/together_model.py\", line 507, in forward\n",
      "    backbone = self._get_bb_reps(pred_rotmats, pred_trans) # [B, l, 6 * max_num_res]\n",
      "  File \"/vast/palmer/home.mccleary/sh2748/protein-frame-flow/models/together_model.py\", line 430, in _get_bb_reps\n",
      "    quats = ru.rot_to_quat(rotmats_t)\n",
      "  File \"/vast/palmer/home.mccleary/sh2748/protein-frame-flow/openfold/utils/rigid_utils.py\", line 226, in rot_to_quat\n",
      "    eigvals, vectors = torch.linalg.eigh(k)\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'LinalgEighBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'LinalgEighBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "loss = mse_loss + auxiliary_loss + kl_div\n",
    "print(loss)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit ('fm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0e2400c1a377b47623d27da16e17b3eddff11c79be96fbe43a1e38910d94694"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
