{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from models.together_model import ProteinVAELLMmodel\n",
    "from data import all_atom\n",
    "# Pytorch lightning imports\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from data.pdb_dataloader import PdbDataModule\n",
    "from models.flow_module import FlowModule\n",
    "from experiments import utils as eu\n",
    "import openfold.utils.rigid_utils as ru\n",
    "import torch.nn.functional as F\n",
    "cfg = OmegaConf.load(\"configs/base.yaml\")\n",
    "_cfg = cfg\n",
    "_data_cfg = cfg.data\n",
    "_exp_cfg = cfg.experiment\n",
    "_datamodule: LightningDataModule = PdbDataModule(_data_cfg)\n",
    "_datamodule.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set stuff up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"configs/base.yaml\")\n",
    "_cfg = cfg\n",
    "_data_cfg = cfg.data\n",
    "_exp_cfg = cfg.experiment\n",
    "training_cfg = _exp_cfg.training\n",
    "_datamodule: LightningDataModule = PdbDataModule(_data_cfg)\n",
    "_datamodule.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = _datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.my_interpolant import Interpolant \n",
    "interpolant = Interpolant(cfg.interpolant)\n",
    "interpolant.set_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProteinVAELLMmodel(cfg).to(\"cuda\")\n",
    "model.attach_backward_hooks() # attach backward hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "            params=model.parameters(),\n",
    "            lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3, 19,  9, 16, 19, 18, 11,  3,  4,  2, 18, 16,  7, 13, 15,  7,  7, 10,\n",
      "         16,  9,  7,  3, 18,  2, 10,  0,  1, 10,  2, 15, 10,  7, 19, 10,  2,  3,\n",
      "          3,  9, 15, 15, 10,  1,  9, 16,  5,  7, 18,  5,  0,  9, 10, 18,  5,  3,\n",
      "          3,  2, 13,  7,  7,  0, 15, 16, 19,  9,  2, 15,  3,  2, 15,  4, 10,  2,\n",
      "         16, 16, 17,  2,  3, 11, 19, 15, 15,  9,  1, 19,  9,  0],\n",
      "        [11, 19, 13,  6, 19,  8, 19,  1, 14, 11, 11, 10,  0, 19,  6, 14, 11,  7,\n",
      "         15, 10,  6, 19,  2,  4, 15, 16, 16,  4,  2,  5, 14,  6, 19,  7,  7, 10,\n",
      "          6, 16, 15, 10,  2, 11,  9, 10, 10,  3,  6,  5,  0,  5, 17, 11,  8, 18,\n",
      "         10, 19, 15,  2,  9, 15,  8,  3, 16, 19, 10,  5,  4,  8, 13, 16,  4, 15,\n",
      "          7, 11,  5,  6, 15, 12,  2, 15,  2, 19, 15, 19, 18,  5]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch[\"aatype\"])\n",
    "for key, value in batch.items():\n",
    "    batch[key] = value.to(\"cuda\")\n",
    "noisy_batch = interpolant.corrupt_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters with NaNs: []\n",
      "Parameters without NaNs: ['framediff_model.node_embedder.linear.weight', 'framediff_model.node_embedder.linear.bias', 'framediff_model.edge_embedder.linear_s_p.weight', 'framediff_model.edge_embedder.linear_s_p.bias', 'framediff_model.edge_embedder.linear_relpos.weight', 'framediff_model.edge_embedder.linear_relpos.bias', 'framediff_model.edge_embedder.edge_embedder.0.weight', 'framediff_model.edge_embedder.edge_embedder.0.bias', 'framediff_model.edge_embedder.edge_embedder.2.weight', 'framediff_model.edge_embedder.edge_embedder.2.bias', 'framediff_model.edge_embedder.edge_embedder.4.weight', 'framediff_model.edge_embedder.edge_embedder.4.bias', 'framediff_model.edge_embedder.edge_embedder.5.weight', 'framediff_model.edge_embedder.edge_embedder.5.bias', 'framediff_model.trunk.ipa_0.head_weights', 'framediff_model.trunk.ipa_0.linear_q.weight', 'framediff_model.trunk.ipa_0.linear_q.bias', 'framediff_model.trunk.ipa_0.linear_kv.weight', 'framediff_model.trunk.ipa_0.linear_kv.bias', 'framediff_model.trunk.ipa_0.linear_q_points.weight', 'framediff_model.trunk.ipa_0.linear_q_points.bias', 'framediff_model.trunk.ipa_0.linear_kv_points.weight', 'framediff_model.trunk.ipa_0.linear_kv_points.bias', 'framediff_model.trunk.ipa_0.linear_b.weight', 'framediff_model.trunk.ipa_0.linear_b.bias', 'framediff_model.trunk.ipa_0.down_z.weight', 'framediff_model.trunk.ipa_0.down_z.bias', 'framediff_model.trunk.ipa_0.linear_out.weight', 'framediff_model.trunk.ipa_0.linear_out.bias', 'framediff_model.trunk.ipa_ln_0.weight', 'framediff_model.trunk.ipa_ln_0.bias', 'framediff_model.trunk.seq_tfmr_0.layers.0.self_attn.in_proj_weight', 'framediff_model.trunk.seq_tfmr_0.layers.0.self_attn.in_proj_bias', 'framediff_model.trunk.seq_tfmr_0.layers.0.self_attn.out_proj.weight', 'framediff_model.trunk.seq_tfmr_0.layers.0.self_attn.out_proj.bias', 'framediff_model.trunk.seq_tfmr_0.layers.0.linear1.weight', 'framediff_model.trunk.seq_tfmr_0.layers.0.linear1.bias', 'framediff_model.trunk.seq_tfmr_0.layers.0.linear2.weight', 'framediff_model.trunk.seq_tfmr_0.layers.0.linear2.bias', 'framediff_model.trunk.seq_tfmr_0.layers.0.norm1.weight', 'framediff_model.trunk.seq_tfmr_0.layers.0.norm1.bias', 'framediff_model.trunk.seq_tfmr_0.layers.0.norm2.weight', 'framediff_model.trunk.seq_tfmr_0.layers.0.norm2.bias', 'framediff_model.trunk.seq_tfmr_0.layers.1.self_attn.in_proj_weight', 'framediff_model.trunk.seq_tfmr_0.layers.1.self_attn.in_proj_bias', 'framediff_model.trunk.seq_tfmr_0.layers.1.self_attn.out_proj.weight', 'framediff_model.trunk.seq_tfmr_0.layers.1.self_attn.out_proj.bias', 'framediff_model.trunk.seq_tfmr_0.layers.1.linear1.weight', 'framediff_model.trunk.seq_tfmr_0.layers.1.linear1.bias', 'framediff_model.trunk.seq_tfmr_0.layers.1.linear2.weight', 'framediff_model.trunk.seq_tfmr_0.layers.1.linear2.bias', 'framediff_model.trunk.seq_tfmr_0.layers.1.norm1.weight', 'framediff_model.trunk.seq_tfmr_0.layers.1.norm1.bias', 'framediff_model.trunk.seq_tfmr_0.layers.1.norm2.weight', 'framediff_model.trunk.seq_tfmr_0.layers.1.norm2.bias', 'framediff_model.trunk.post_tfmr_0.weight', 'framediff_model.trunk.post_tfmr_0.bias', 'framediff_model.trunk.node_transition_0.linear_1.weight', 'framediff_model.trunk.node_transition_0.linear_1.bias', 'framediff_model.trunk.node_transition_0.linear_2.weight', 'framediff_model.trunk.node_transition_0.linear_2.bias', 'framediff_model.trunk.node_transition_0.linear_3.weight', 'framediff_model.trunk.node_transition_0.linear_3.bias', 'framediff_model.trunk.node_transition_0.ln.weight', 'framediff_model.trunk.node_transition_0.ln.bias', 'framediff_model.trunk.bb_update_0.linear.weight', 'framediff_model.trunk.bb_update_0.linear.bias', 'framediff_model.trunk.edge_transition_0.initial_embed.weight', 'framediff_model.trunk.edge_transition_0.initial_embed.bias', 'framediff_model.trunk.edge_transition_0.trunk.0.weight', 'framediff_model.trunk.edge_transition_0.trunk.0.bias', 'framediff_model.trunk.edge_transition_0.trunk.2.weight', 'framediff_model.trunk.edge_transition_0.trunk.2.bias', 'framediff_model.trunk.edge_transition_0.final_layer.weight', 'framediff_model.trunk.edge_transition_0.final_layer.bias', 'framediff_model.trunk.edge_transition_0.layer_norm.weight', 'framediff_model.trunk.edge_transition_0.layer_norm.bias', 'framediff_model.trunk.ipa_1.head_weights', 'framediff_model.trunk.ipa_1.linear_q.weight', 'framediff_model.trunk.ipa_1.linear_q.bias', 'framediff_model.trunk.ipa_1.linear_kv.weight', 'framediff_model.trunk.ipa_1.linear_kv.bias', 'framediff_model.trunk.ipa_1.linear_q_points.weight', 'framediff_model.trunk.ipa_1.linear_q_points.bias', 'framediff_model.trunk.ipa_1.linear_kv_points.weight', 'framediff_model.trunk.ipa_1.linear_kv_points.bias', 'framediff_model.trunk.ipa_1.linear_b.weight', 'framediff_model.trunk.ipa_1.linear_b.bias', 'framediff_model.trunk.ipa_1.down_z.weight', 'framediff_model.trunk.ipa_1.down_z.bias', 'framediff_model.trunk.ipa_1.linear_out.weight', 'framediff_model.trunk.ipa_1.linear_out.bias', 'framediff_model.trunk.ipa_ln_1.weight', 'framediff_model.trunk.ipa_ln_1.bias', 'framediff_model.trunk.seq_tfmr_1.layers.0.self_attn.in_proj_weight', 'framediff_model.trunk.seq_tfmr_1.layers.0.self_attn.in_proj_bias', 'framediff_model.trunk.seq_tfmr_1.layers.0.self_attn.out_proj.weight', 'framediff_model.trunk.seq_tfmr_1.layers.0.self_attn.out_proj.bias', 'framediff_model.trunk.seq_tfmr_1.layers.0.linear1.weight', 'framediff_model.trunk.seq_tfmr_1.layers.0.linear1.bias', 'framediff_model.trunk.seq_tfmr_1.layers.0.linear2.weight', 'framediff_model.trunk.seq_tfmr_1.layers.0.linear2.bias', 'framediff_model.trunk.seq_tfmr_1.layers.0.norm1.weight', 'framediff_model.trunk.seq_tfmr_1.layers.0.norm1.bias', 'framediff_model.trunk.seq_tfmr_1.layers.0.norm2.weight', 'framediff_model.trunk.seq_tfmr_1.layers.0.norm2.bias', 'framediff_model.trunk.seq_tfmr_1.layers.1.self_attn.in_proj_weight', 'framediff_model.trunk.seq_tfmr_1.layers.1.self_attn.in_proj_bias', 'framediff_model.trunk.seq_tfmr_1.layers.1.self_attn.out_proj.weight', 'framediff_model.trunk.seq_tfmr_1.layers.1.self_attn.out_proj.bias', 'framediff_model.trunk.seq_tfmr_1.layers.1.linear1.weight', 'framediff_model.trunk.seq_tfmr_1.layers.1.linear1.bias', 'framediff_model.trunk.seq_tfmr_1.layers.1.linear2.weight', 'framediff_model.trunk.seq_tfmr_1.layers.1.linear2.bias', 'framediff_model.trunk.seq_tfmr_1.layers.1.norm1.weight', 'framediff_model.trunk.seq_tfmr_1.layers.1.norm1.bias', 'framediff_model.trunk.seq_tfmr_1.layers.1.norm2.weight', 'framediff_model.trunk.seq_tfmr_1.layers.1.norm2.bias', 'framediff_model.trunk.post_tfmr_1.weight', 'framediff_model.trunk.post_tfmr_1.bias', 'framediff_model.trunk.node_transition_1.linear_1.weight', 'framediff_model.trunk.node_transition_1.linear_1.bias', 'framediff_model.trunk.node_transition_1.linear_2.weight', 'framediff_model.trunk.node_transition_1.linear_2.bias', 'framediff_model.trunk.node_transition_1.linear_3.weight', 'framediff_model.trunk.node_transition_1.linear_3.bias', 'framediff_model.trunk.node_transition_1.ln.weight', 'framediff_model.trunk.node_transition_1.ln.bias', 'framediff_model.trunk.bb_update_1.linear.weight', 'framediff_model.trunk.bb_update_1.linear.bias', 'vaellm_model.llm_model.wpe.weight', 'vaellm_model.llm_model.h.0.ln_1.weight', 'vaellm_model.llm_model.h.0.ln_1.bias', 'vaellm_model.llm_model.h.0.attn.c_attn.weight', 'vaellm_model.llm_model.h.0.attn.c_attn.bias', 'vaellm_model.llm_model.h.0.attn.c_proj.weight', 'vaellm_model.llm_model.h.0.attn.c_proj.bias', 'vaellm_model.llm_model.h.0.ln_2.weight', 'vaellm_model.llm_model.h.0.ln_2.bias', 'vaellm_model.llm_model.h.0.mlp.c_fc.weight', 'vaellm_model.llm_model.h.0.mlp.c_fc.bias', 'vaellm_model.llm_model.h.0.mlp.c_proj.weight', 'vaellm_model.llm_model.h.0.mlp.c_proj.bias', 'vaellm_model.llm_model.h.1.ln_1.weight', 'vaellm_model.llm_model.h.1.ln_1.bias', 'vaellm_model.llm_model.h.1.attn.c_attn.weight', 'vaellm_model.llm_model.h.1.attn.c_attn.bias', 'vaellm_model.llm_model.h.1.attn.c_proj.weight', 'vaellm_model.llm_model.h.1.attn.c_proj.bias', 'vaellm_model.llm_model.h.1.ln_2.weight', 'vaellm_model.llm_model.h.1.ln_2.bias', 'vaellm_model.llm_model.h.1.mlp.c_fc.weight', 'vaellm_model.llm_model.h.1.mlp.c_fc.bias', 'vaellm_model.llm_model.h.1.mlp.c_proj.weight', 'vaellm_model.llm_model.h.1.mlp.c_proj.bias', 'vaellm_model.llm_model.h.2.ln_1.weight', 'vaellm_model.llm_model.h.2.ln_1.bias', 'vaellm_model.llm_model.h.2.attn.c_attn.weight', 'vaellm_model.llm_model.h.2.attn.c_attn.bias', 'vaellm_model.llm_model.h.2.attn.c_proj.weight', 'vaellm_model.llm_model.h.2.attn.c_proj.bias', 'vaellm_model.llm_model.h.2.ln_2.weight', 'vaellm_model.llm_model.h.2.ln_2.bias', 'vaellm_model.llm_model.h.2.mlp.c_fc.weight', 'vaellm_model.llm_model.h.2.mlp.c_fc.bias', 'vaellm_model.llm_model.h.2.mlp.c_proj.weight', 'vaellm_model.llm_model.h.2.mlp.c_proj.bias', 'vaellm_model.llm_model.h.3.ln_1.weight', 'vaellm_model.llm_model.h.3.ln_1.bias', 'vaellm_model.llm_model.h.3.attn.c_attn.weight', 'vaellm_model.llm_model.h.3.attn.c_attn.bias', 'vaellm_model.llm_model.h.3.attn.c_proj.weight', 'vaellm_model.llm_model.h.3.attn.c_proj.bias', 'vaellm_model.llm_model.h.3.ln_2.weight', 'vaellm_model.llm_model.h.3.ln_2.bias', 'vaellm_model.llm_model.h.3.mlp.c_fc.weight', 'vaellm_model.llm_model.h.3.mlp.c_fc.bias', 'vaellm_model.llm_model.h.3.mlp.c_proj.weight', 'vaellm_model.llm_model.h.3.mlp.c_proj.bias', 'vaellm_model.llm_model.h.4.ln_1.weight', 'vaellm_model.llm_model.h.4.ln_1.bias', 'vaellm_model.llm_model.h.4.attn.c_attn.weight', 'vaellm_model.llm_model.h.4.attn.c_attn.bias', 'vaellm_model.llm_model.h.4.attn.c_proj.weight', 'vaellm_model.llm_model.h.4.attn.c_proj.bias', 'vaellm_model.llm_model.h.4.ln_2.weight', 'vaellm_model.llm_model.h.4.ln_2.bias', 'vaellm_model.llm_model.h.4.mlp.c_fc.weight', 'vaellm_model.llm_model.h.4.mlp.c_fc.bias', 'vaellm_model.llm_model.h.4.mlp.c_proj.weight', 'vaellm_model.llm_model.h.4.mlp.c_proj.bias', 'vaellm_model.llm_model.h.5.ln_1.weight', 'vaellm_model.llm_model.h.5.ln_1.bias', 'vaellm_model.llm_model.h.5.attn.c_attn.weight', 'vaellm_model.llm_model.h.5.attn.c_attn.bias', 'vaellm_model.llm_model.h.5.attn.c_proj.weight', 'vaellm_model.llm_model.h.5.attn.c_proj.bias', 'vaellm_model.llm_model.h.5.ln_2.weight', 'vaellm_model.llm_model.h.5.ln_2.bias', 'vaellm_model.llm_model.h.5.mlp.c_fc.weight', 'vaellm_model.llm_model.h.5.mlp.c_fc.bias', 'vaellm_model.llm_model.h.5.mlp.c_proj.weight', 'vaellm_model.llm_model.h.5.mlp.c_proj.bias', 'vaellm_model.llm_model.h.6.ln_1.weight', 'vaellm_model.llm_model.h.6.ln_1.bias', 'vaellm_model.llm_model.h.6.attn.c_attn.weight', 'vaellm_model.llm_model.h.6.attn.c_attn.bias', 'vaellm_model.llm_model.h.6.attn.c_proj.weight', 'vaellm_model.llm_model.h.6.attn.c_proj.bias', 'vaellm_model.llm_model.h.6.ln_2.weight', 'vaellm_model.llm_model.h.6.ln_2.bias', 'vaellm_model.llm_model.h.6.mlp.c_fc.weight', 'vaellm_model.llm_model.h.6.mlp.c_fc.bias', 'vaellm_model.llm_model.h.6.mlp.c_proj.weight', 'vaellm_model.llm_model.h.6.mlp.c_proj.bias', 'vaellm_model.llm_model.h.7.ln_1.weight', 'vaellm_model.llm_model.h.7.ln_1.bias', 'vaellm_model.llm_model.h.7.attn.c_attn.weight', 'vaellm_model.llm_model.h.7.attn.c_attn.bias', 'vaellm_model.llm_model.h.7.attn.c_proj.weight', 'vaellm_model.llm_model.h.7.attn.c_proj.bias', 'vaellm_model.llm_model.h.7.ln_2.weight', 'vaellm_model.llm_model.h.7.ln_2.bias', 'vaellm_model.llm_model.h.7.mlp.c_fc.weight', 'vaellm_model.llm_model.h.7.mlp.c_fc.bias', 'vaellm_model.llm_model.h.7.mlp.c_proj.weight', 'vaellm_model.llm_model.h.7.mlp.c_proj.bias', 'vaellm_model.llm_model.h.8.ln_1.weight', 'vaellm_model.llm_model.h.8.ln_1.bias', 'vaellm_model.llm_model.h.8.attn.c_attn.weight', 'vaellm_model.llm_model.h.8.attn.c_attn.bias', 'vaellm_model.llm_model.h.8.attn.c_proj.weight', 'vaellm_model.llm_model.h.8.attn.c_proj.bias', 'vaellm_model.llm_model.h.8.ln_2.weight', 'vaellm_model.llm_model.h.8.ln_2.bias', 'vaellm_model.llm_model.h.8.mlp.c_fc.weight', 'vaellm_model.llm_model.h.8.mlp.c_fc.bias', 'vaellm_model.llm_model.h.8.mlp.c_proj.weight', 'vaellm_model.llm_model.h.8.mlp.c_proj.bias', 'vaellm_model.llm_model.h.9.ln_1.weight', 'vaellm_model.llm_model.h.9.ln_1.bias', 'vaellm_model.llm_model.h.9.attn.c_attn.weight', 'vaellm_model.llm_model.h.9.attn.c_attn.bias', 'vaellm_model.llm_model.h.9.attn.c_proj.weight', 'vaellm_model.llm_model.h.9.attn.c_proj.bias', 'vaellm_model.llm_model.h.9.ln_2.weight', 'vaellm_model.llm_model.h.9.ln_2.bias', 'vaellm_model.llm_model.h.9.mlp.c_fc.weight', 'vaellm_model.llm_model.h.9.mlp.c_fc.bias', 'vaellm_model.llm_model.h.9.mlp.c_proj.weight', 'vaellm_model.llm_model.h.9.mlp.c_proj.bias', 'vaellm_model.llm_model.h.10.ln_1.weight', 'vaellm_model.llm_model.h.10.ln_1.bias', 'vaellm_model.llm_model.h.10.attn.c_attn.weight', 'vaellm_model.llm_model.h.10.attn.c_attn.bias', 'vaellm_model.llm_model.h.10.attn.c_proj.weight', 'vaellm_model.llm_model.h.10.attn.c_proj.bias', 'vaellm_model.llm_model.h.10.ln_2.weight', 'vaellm_model.llm_model.h.10.ln_2.bias', 'vaellm_model.llm_model.h.10.mlp.c_fc.weight', 'vaellm_model.llm_model.h.10.mlp.c_fc.bias', 'vaellm_model.llm_model.h.10.mlp.c_proj.weight', 'vaellm_model.llm_model.h.10.mlp.c_proj.bias', 'vaellm_model.llm_model.h.11.ln_1.weight', 'vaellm_model.llm_model.h.11.ln_1.bias', 'vaellm_model.llm_model.h.11.attn.c_attn.weight', 'vaellm_model.llm_model.h.11.attn.c_attn.bias', 'vaellm_model.llm_model.h.11.attn.c_proj.weight', 'vaellm_model.llm_model.h.11.attn.c_proj.bias', 'vaellm_model.llm_model.h.11.ln_2.weight', 'vaellm_model.llm_model.h.11.ln_2.bias', 'vaellm_model.llm_model.h.11.mlp.c_fc.weight', 'vaellm_model.llm_model.h.11.mlp.c_fc.bias', 'vaellm_model.llm_model.h.11.mlp.c_proj.weight', 'vaellm_model.llm_model.h.11.mlp.c_proj.bias', 'vaellm_model.llm_model.ln_f.weight', 'vaellm_model.llm_model.ln_f.bias', 'vaellm_model.hid2mu.weight', 'vaellm_model.hid2mu.bias', 'vaellm_model.hid2sigma.weight', 'vaellm_model.hid2sigma.bias', 'lin_emb_backbone.weight', 'lin_emb_backbone.bias', 'lin_deembed_backbone.weight', 'lin_deembed_backbone.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1554: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1554: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'dict'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1554: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'dict'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/gpfs/gibbs/project/dijk/sh2748/conda_envs/fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1554: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'dict'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    }
   ],
   "source": [
    "B, l, num_res = noisy_batch['res_mask'].shape\n",
    "loss_mask = noisy_batch['res_mask'].reshape(B*l, num_res)\n",
    "        \n",
    "if training_cfg.min_plddt_mask is not None:\n",
    "    plddt_mask = noisy_batch['res_plddt'] > training_cfg.min_plddt_mask\n",
    "    loss_mask *= plddt_mask\n",
    "        \n",
    "\n",
    "# Ground truth labels\n",
    "gt_trans = noisy_batch['trans_t']\n",
    "gt_rotmats = noisy_batch['rotmats_t'] \n",
    "\n",
    "# Model output predictions.\n",
    "\n",
    "framediff_out = model(noisy_batch)\n",
    "pred_trans = framediff_out[\"pred_T\"]['pred_trans'].reshape(B, l, 128, 3)\n",
    "pred_rotmats = framediff_out[\"pred_T\"]['pred_rotmats'].reshape(B, l, 128, 3, 3)\n",
    "        \n",
    "# Shift for CausalLM\n",
    "shifted_pred_trans  = pred_trans[:, :-1, :, :]\n",
    "shifted_pred_rotmats = pred_rotmats[:, :-1, :, :, :]\n",
    "shifted_gt_trans = gt_trans[:, 1:, :, :]\n",
    "shifted_gt_rotmats = gt_rotmats[:, 1:, :, :, :]\n",
    "\n",
    "# Reshape back to [B*(l-1), 128, *]\n",
    "flat_shifted_pred_trans = shifted_pred_trans.reshape(B*(l-1), _data_cfg.dataset.max_num_res, 3)\n",
    "flat_shifted_pred_rotmats = shifted_pred_rotmats.reshape(B*(l-1), _data_cfg.dataset.max_num_res, 3, 3)\n",
    "\n",
    "flat_shifted_gt_trans = shifted_gt_trans.reshape(B*(l-1), _data_cfg.dataset.max_num_res, 3)\n",
    "flat_shifted_gt_rotmats = shifted_gt_rotmats.reshape(B*(l-1), _data_cfg.dataset.max_num_res, 3, 3)\n",
    "\n",
    "# Timestep used for normalization.\n",
    "t = noisy_batch['t'].reshape(B, l, 1)[:, 1:, :].reshape(-1,1) # We throw away the first time points\n",
    "norm_scale = 1 - torch.min(\n",
    "    t[..., None], torch.tensor(training_cfg.t_normalize_clip))\n",
    "        \n",
    "        \n",
    "\n",
    "# Backbone atom loss\n",
    "gt_bb_atoms = all_atom.to_atom37(flat_shifted_gt_trans, flat_shifted_gt_rotmats)[:, :, :3] \n",
    "pred_bb_atoms = all_atom.to_atom37(flat_shifted_pred_trans, flat_shifted_pred_rotmats)[:, :, :3]\n",
    "\n",
    "gt_bb_atoms *= training_cfg.bb_atom_scale / norm_scale[..., None]\n",
    "pred_bb_atoms *= training_cfg.bb_atom_scale / norm_scale[..., None]\n",
    "                \n",
    "        \n",
    "loss_denom = torch.sum(loss_mask, dim=-1, dtype=torch.float).mean() * 3 # Added a mean here, this doesn'y matter since our mask is all 1's\n",
    "bb_atom_loss = torch.sum(\n",
    "    (gt_bb_atoms - pred_bb_atoms) ** 2,\n",
    "    dim=(-1, -2, -3)\n",
    ") / loss_denom\n",
    "\n",
    "# Pairwise distance loss\n",
    "num_batch = gt_bb_atoms.shape[0]\n",
    "gt_flat_atoms = gt_bb_atoms.reshape([num_batch, num_res*3, 3])\n",
    "gt_pair_dists = torch.linalg.norm(\n",
    "    gt_flat_atoms[:, :, None, :] - gt_flat_atoms[:, None, :, :], dim=-1)\n",
    "pred_flat_atoms = pred_bb_atoms.reshape([num_batch, num_res*3, 3])\n",
    "pred_pair_dists = torch.linalg.norm(\n",
    "    pred_flat_atoms[:, :, None, :] - pred_flat_atoms[:, None, :, :], dim=-1)\n",
    "\n",
    "flat_loss_mask = torch.tile(loss_mask[:, :, None], (1, 1, 3))[B:,:,:] # change the shape because we shifted tokens, all entries of loss masks are 1 so don't matter, we throw away B tokens\n",
    "flat_loss_mask = flat_loss_mask.reshape([num_batch, num_res*3])\n",
    "flat_res_mask = torch.tile(loss_mask[:, :, None], (1, 1, 3))[B:,:,:] # change the shape because we shifted tokens, all entries of loss masks are 1 so don't matter\n",
    "flat_res_mask = flat_res_mask.reshape([num_batch, num_res*3])\n",
    "\n",
    "gt_pair_dists = gt_pair_dists * flat_loss_mask[..., None]\n",
    "pred_pair_dists = pred_pair_dists * flat_loss_mask[..., None]\n",
    "pair_dist_mask = flat_loss_mask[..., None] * flat_res_mask[:, None, :]\n",
    "\n",
    "dist_mat_loss = torch.sum(\n",
    "    (gt_pair_dists - pred_pair_dists)**2 * pair_dist_mask,\n",
    "    dim=(1, 2))\n",
    "dist_mat_loss /= (torch.sum(pair_dist_mask, dim=(1, 2)) - num_res)\n",
    "\n",
    "auxiliary_loss = (bb_atom_loss + dist_mat_loss) * (\n",
    "    t[:, 0]> training_cfg.aux_loss_t_pass\n",
    ")\n",
    "auxiliary_loss *= _exp_cfg.training.aux_loss_weight\n",
    "        \n",
    "auxiliary_loss = auxiliary_loss.mean()\n",
    "kl_div = (1 + 2 * framediff_out[\"vae_log_sigma\"] - framediff_out[\"vae_mu\"].pow(2) - framediff_out[\"vae_log_sigma\"].exp().pow(2))[:, :-1, :] # Throw away the last mu and sigma because we're not using it to predict\n",
    "kl_div = - 0.5 * kl_div.sum(dim=-1).mean()\n",
    "mse_loss = F.mse_loss(flat_shifted_pred_trans, flat_shifted_gt_trans) + F.mse_loss(flat_shifted_pred_rotmats, flat_shifted_gt_rotmats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse_loss + auxiliary_loss + kl_div\n",
    "print(loss)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit ('fm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0e2400c1a377b47623d27da16e17b3eddff11c79be96fbe43a1e38910d94694"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
